<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/my-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/my-blog/" rel="alternate" type="text/html" /><updated>2021-01-29T22:25:53+00:00</updated><id>http://localhost:4000/my-blog/feed.xml</id><title type="html">ARIZE-BLOG</title><subtitle>Welcome to Arize blog.</subtitle><author><name>Arize Emmanuel</name></author><entry><title type="html">Logistic Regression</title><link href="http://localhost:4000/my-blog/deeplearning/deep-learning/2021/01/24/Logistic-Regression.html" rel="alternate" type="text/html" title="Logistic Regression" /><published>2021-01-24T00:00:00+00:00</published><updated>2021-01-24T00:00:00+00:00</updated><id>http://localhost:4000/my-blog/deeplearning/deep-learning/2021/01/24/Logistic-Regression</id><content type="html" xml:base="http://localhost:4000/my-blog/deeplearning/deep-learning/2021/01/24/Logistic-Regression.html">&lt;p&gt;In the previous section [&lt;a href=&quot;/my-blog/deeplearning/deep-learning/2021/01/16/Linear-Regression.html&quot;&gt;Linear Regression&lt;/a&gt;], we talked about linear regression which deals with quatitative target variable, answering the questions how much? such as predicting the price of a house and  the salary of an employee. In practice, we are more often interested in making categorical assignments by mapping an instance to a category of the target variable which may contain two or more different categories. Classification problems ask the question not how much? but which one (group or class) does an instance belongs to such as a boy or a girl. Classification problems include Spam e-mail filtering in which spam is taken as the positive class and ham as the negative class, comments classification where comments are classified as either positive or negative,  medical diagnosis where having a particular disease is assigned the positive class and credit card fraud detection. We consider a problem to be a classification problem when dealing with qualitative (categorical) target variable with the aim of assigning an input described by vector \(X_{i}\) to one of the n discrete categories (classes) \(C_{i}\)  where \(i = 1,2 \cdots,n\).&lt;/p&gt;

&lt;p&gt;A classifier is a map \(f(X_{i}) \rightarrow C_{i}\)&lt;/p&gt;

&lt;p&gt;There are numerous classification methods that can be used to predict categorical outcomes including support vector machines (SVM) and decision trees  but this tutorial is based on logistic regression (neural network).&lt;/p&gt;

&lt;p&gt; Before describing logistic regression since we need some packages for the implementation of the model, the code below imports the needed packages for this tutorials &lt;/p&gt;
&lt;div class=&quot;w3-container&quot;&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
from sklearn.datasets import load_iris
import torch
import torch.nn as nn
import pandas as pd
from torch.utils.data import DataLoader,TensorDataset
import numpy as np
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
from sklearn.datasets import load_iris
import pandas as pd
import mxnet
from mxnet import autograd,np,npx,gluon,nd
from mxnet.gluon import nn
npx.set_np()&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers,Model
from tensorflow import keras&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;NOTE:    In this tutorial the famous iris dataset will be used.
     &lt;strong&gt;Description of the dataset&lt;/strong&gt; . –
     The famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let now load the dataset and display the first three rows of the input features&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/logistic/threerows.jpg&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p&gt; The code below convert the arrays into a dataframe, concatenate both the target and the input features into a single dataframe and display the last five rows.
&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# converting the input features into a dataframe
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# converting the target variable into a dataframe
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'species'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# concatinating both the target and the input features into a 
#single dataframe
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'species'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'species'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/logistic/last5rows.jpg&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Since the target variable has 3 species (three classes) of iris namely Iris setosa (0), versicolor(1), and virginica (2), the code below convert the data into a binary problem by dropping all virginica (2) observations&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'species'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;&lt;b&gt; Logistic Regression&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Logistic regression is one of the most popular classification methods.
Although it’s named regression and has the same underlying method as that of linear regression, it is not a regression method but rather a classification method. The simplest classification problem involves problems in which the target variable contain only two categories which are usually labeled &lt;b&gt;1&lt;/b&gt; for the positive class (y=1|x) and  &lt;b&gt;O&lt;/b&gt; for negative class (y=0|x) and are often called &lt;b&gt;binary classification&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Logistic regression&lt;/b&gt; is a probabilistic binary classifier which estimates, for each data point, the conditional probability that it belongs to one of the categories of the target variable. It can be used to classify an observation into one of the two categories from a set of both continuous or categorical predictor variables. By setting a threshold &lt;b&gt;r&lt;/b&gt;, we classified output with probability greater than the threshold as one class usually the class labelled &lt;b&gt;1&lt;/b&gt; and values below the threshold as belonging to the class labelled &lt;b&gt;0&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Recall from linear regression&lt;/p&gt;

\[\hat{y}= w^{T}x+b\]

&lt;p&gt;&lt;br /&gt;
Since we are interested in mapping an instance to a class which is either 0 or 1, we want the model to predict the probability of a instance as either belonging to class 0 or 1, so instead of output from the linear regression model which can have values less than \(0\) or greater than \(1\), we will modify the output by running the output from the linear function through a logistic sigmoid activation function \(\sigma\) to output values within the range of [0,1]. Using the sigmoid function, it first computes the real-valued score from \(w^{T}x+b\) and then squashes it between [0,1] to turn the score into a probability score.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note:   The sigmoid function \(\sigma\) is sometimes called  logistic function&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Logistic function is defined as&lt;/p&gt;

\[\hat{y} =\sigma(z)\]

&lt;p&gt;&lt;br /&gt;
where&lt;/p&gt;

\[\sigma(z)=\frac{1}{1+\exp(-z)}\]

&lt;p&gt;and z is the linear function consisting of the input data and their associated weights and bias. Thus&lt;/p&gt;

\[z=\sum_{i=1}^{n}W_{i}^{T}X_{i} +b_{i}\]

&lt;p&gt;&lt;br /&gt;
Setting the threshold at r, the decision rule is of the form&lt;/p&gt;

\[\hat y =
  \begin{cases}
    1       &amp;amp; \quad \text{if } \sigma(z)  \text{ } \geq \text{r}\\
    0  &amp;amp; \quad \text{if } \sigma(z) \text{} &amp;lt; \text{r} 
  \end{cases}\]

&lt;p&gt;&lt;br /&gt;
The model then uses the learned parameters (weights and bias) from the training data, to make a classification on newly unseen instance or example. Each input feature has an associated weight.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Let now define the logistic model and initialized it parameters
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;w3-container&quot;&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
def sigmoid(z):
    return 1/(1+torch.exp(-z))

class Logistic_Regression1(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1=nn.Linear(4,1)

    def forward(self,x):
        linear=self.layer1(x)
        return sigmoid(linear)
model=Logistic_Regression1()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
def sigmoid(z):
    return 1/(1+np.exp(-z))

class Logistic_Regression(nn.Block):
    def __init__(self):
        super().__init__()
        with self.name_scope():
            self.w=self.params.get('weight',
                   init=mxnet.init.Normal(sigma=0.5),shape=(4,1))
            self.b=self.params.get(&quot;bias&quot;,
                    init=mxnet.init.Normal(sigma=0.5),shape=(1))
    def forward(self,x):
        linear=np.dot(x,self.w.data())+self.b.data()
        return sigmoid(linear)
model=Logistic_Regression()
model.initialize()
        &lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
def sigmoid(z):
    return 1/(1+tf.exp(-z))

class Linear(layers.Layer):
    def __init__(self):
        super().__init__()
        self.w=self.add_weight(shape=(4,1),
                               initializer=&quot;random_normal&quot;,
                               trainable=True)
        self.b=self.add_weight(shape=(1,),
                               initializer=&quot;zeros&quot;,
                               trainable=True)
    def call(self,x):
        return tf.matmul(x,self.w)+self.b
    
class Logistic_Regression(Model):
    def __init__(self):
        super().__init__()
        self.linear=Linear()
    def call(self,inputs):
        linear=self.linear(inputs)
        return sigmoid(linear)

model=Logistic_Regression()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To determine how good or bad the model is generalizing, we need a performance measure to evaluate the model.&lt;/p&gt;

&lt;h3&gt;&lt;b&gt; Loss (Cost or Objective) Function&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;In classification problem where we are interested in assigning an observation to a class label and as such we need a loss function to measure the difference between the model’s predicted class and the actual class. Since only two discrete outcomes (categories) are involved, using the maximum likelihood principle, the optimal model parameters w and b are those that maximize the likelihood of the entire dataset. For a dataset of \((x_{i},y_{i})\) where  \(y_{i} \in \{0,1\}\) the likelihood function can be written as&lt;/p&gt;

\[P(y|x,w)=\prod_{i}^{n} \hat y^{y_{i}} (1-\hat y)^{1-y_{i}}\]

&lt;p&gt;&lt;br /&gt;
Maximizing the product of the exponential function, might look difficult so without changing the loss function we can simplify things by maximizing the log  likelihood of \(P(y|x,w)\) which requires taking the log on both sides of the equation &lt;br /&gt;&lt;/p&gt;

\[\log{P(y|x,w)}=log ( \hat y^{y_{i}} (1-\hat y)^{1-y_{i}}) =y_{i}log \hat y + (1-y_{i})log(1 -\hat y)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Since maximization problem is equivalent to minimizing the  negative log likelihood (NLL) and Obtaining optimal model parameters involves minimizing the loss function, so instead of maximizing, we will minimize the loss function (cost or objective)
 by negating the log likelihood defined by&lt;br /&gt;&lt;/p&gt;

\[\ell(w)=-\sum_{i}^{n}(y_{i}log \hat y + (1-y_{i})log(1 -\hat y))\]

&lt;p&gt;&lt;br /&gt;
and is known as the &lt;b&gt; binary cross entropy&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Binary cross entropy&lt;/b&gt; will be our lost function and is defined as&lt;/p&gt;

\[L(y,\hat{y})=-\frac{1}{n}\sum_{i=1}^{n}(y_{i}log \hat y + (1-y_{i})log(1 -\hat y) )\]

&lt;p&gt; The code below defines binary cross entropy loss function&lt;/p&gt;

&lt;div class=&quot;w3-container&quot;&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
loss=nn.BCELoss()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
loss=gluon.loss.SigmoidBCELoss()
        &lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
loss=keras.losses.BinaryCrossentropy()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;&lt;b&gt;Updating Model Parameters&lt;/b&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;b&gt; Gradient Descent&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;Gradient Descent is a generic optimization algorithm capable of finding the optimal solutions to a wide range of problems with th goal of iteratively reducing the error by tweaking (updating) the parameters in the direction that incrementally lowers the loss function. During trainiing we want to automatically update the model parameters in order to the find the best parameters that minimize the error. With&lt;/p&gt;

\[\hat{y}=\frac{1}{1+\exp(-z)}\]

&lt;p&gt;&lt;br /&gt; where  \(z=XW+b\) and the loss function defined to be&lt;/p&gt;

\[L(y,\hat{y})=-\frac{1}{n}\sum_{i=1}^{n}(y_{i}log \hat y + (1-y_{i})log(1 -\hat y) )\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;updating the parameter W using stochastic gradient descent(SGD) will be 
&lt;br /&gt;&lt;/p&gt;

\[W_{i+1}=W_{i}- \frac{\beta \frac{\partial L}{\partial W}}{n}\]

&lt;p&gt;where
&lt;br /&gt;&lt;/p&gt;

\[\frac{\partial{L}}{\partial{W}}=\frac{\partial{L}}{\partial \hat y} \frac{\partial \hat y}{\partial z} \frac{\partial z}{\partial W}\]

&lt;p&gt;&lt;br /&gt;
 &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;\(\frac{\partial L}{\partial \hat y}= -y \frac{1}{\hat y}-(1-y)\frac{1}{(1-\hat y)}\),    \(\frac{\partial \hat y}{\partial z}=\frac{exp(-z)}{(1+exp(-z))^{2}}=\hat y(1- \hat y)\)&lt;/p&gt;

&lt;p&gt;,        \(\frac{\partial z}{\partial W}=x\)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The code below defines SGD with the learning rate as 0.05&lt;/p&gt;

&lt;div class=&quot;w3-container&quot;&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
optimizer=torch.optim.SGD(model.parameters(),lr = 0.05 )
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
optimizer=gluon.Trainer(model.collect_params(),&quot;sgd&quot;,
                      {'learning_rate':0.05})
        &lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
optimizer=keras.optimizers.SGD(learning_rate=0.05))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;&lt;b&gt;Feature Scaling or transformation&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Numerical features are often measured on different scales and this variation in scale may pose problems to modelling the data correclty. With few exceptions, variations in scales of numerical features often lead to machine Learning algorithms not performing well. When features have different scales, features with higher magnitude are likely to have higher weights and this affects the performance of the model.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Feature scaling is a technique applied as part of the data preparation process in machine learning to put the numerical features on a common scale without distorting the differences in the range of values.&lt;/em&gt;
There are many feature scaling techniques but We will only discuss &lt;b&gt;Standardization(Z-score)&lt;/b&gt;&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;  Standardization(Z-score)&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;When Z-score is applied to a input feature makes the  feature have a zero mean by subtracting the mean of the feature from the data points and then it divides by the standard deviation so that the resulting distribution has unit variance.&lt;/p&gt;

\[x_{z-score}= \frac{x-\bar{x}}{ \sigma}\]

&lt;p&gt;since all features in our dataset are numrical we are going to scale them using Z-score&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Note: the target values is generally not scaled&lt;/b&gt;&lt;/p&gt;
&lt;p&gt; The code below scaled the data, shuffle it and convert it to tensors&lt;/p&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
train_data.iloc[:,:-1]=train_data.iloc[:,:-1].apply(lambda x: 
                           (x-np.mean(x))/np.std(x))
train_data=np.array(train_data)
np.random.shuffle(train_data)
input_features=train_data[:,:4]
labels=train_data[:,4]
input_features=torch.FloatTensor(input_features)
labels=torch.FloatTensor(labels)
&lt;/code&gt;
&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
train_data.iloc[:,:-1]=train_data.iloc[:,:-1].apply(lambda x:
                                  (x-x.mean())/x.std())
train_data=np.array(train_data)
np.random.shuffle(train_data)
input_features=train_data[:,:4]
labels=train_data[:,4]&lt;/code&gt;
&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
train_data.iloc[:,:-1]=train_data.iloc[:,:-1].apply(lambda x: 
                                    (x-np.mean(x))/np.std(x))
train_data=np.array(train_data)
np.random.shuffle(train_data)
input_features=train_data[:,:4]
labels=train_data[:,4]
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;&lt;b&gt; Data iterators&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;To update parameters we need to iterate through our data points and grab batches of size 30 of the data points at a time and used these batches to update the parameters&lt;br /&gt;&lt;/p&gt;

&lt;div&gt; 
&lt;br /&gt;
&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
def data_iter(features,labels,batch_size):
    dataset=TensorDataset(*(features,labels))
    dataloader=DataLoader(dataset=dataset,batch_size=batch_size,
                           shuffle=True)
    return dataloader
batch_size=30
data_iter=data_iter(input_features,labels,batch_size)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
def data_iter(data_arrays, batch_size):
    dataset=gluon.data.ArrayDataset(*data_arrays)
    dataloader=gluon.data.DataLoader(dataset,batch_size,shuffle=True)
    return dataloader
batch_size=30
data_iter = data_iter((input_features,labels), batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
def data_iter(features,labels, batch_size):
    dataset=tf.data.Dataset.from_tensor_slices((features,labels))
    dataloader=dataset.shuffle(buffer_size=500).batch(batch_size)
    return dataloader
batch_size=30
data_iter = data_iter(input_features,labels, batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;
 &lt;/div&gt;

&lt;p&gt; We finally train the logistic regression model&lt;/p&gt;
&lt;div class=&quot;w3-container&quot;&gt;
&lt;br /&gt;&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/logistic/pytorch.jpg&quot; /&gt;
&lt;/details&gt;&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;img class=&quot;w3-border&quot; src=&quot;/my-blog/assets/images/deep/logistic/mxnet.jpg&quot; /&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
num_epochs = 20
for epoch in range(num_epochs+1): 
    for X,y in data_iter:
        with tf.GradientTape() as tape:
            y_hat=model(X)
            loss_value=loss(y_hat,y)
        gradient=tape.gradient(loss_value,model.trainable_weights)
        optimizer.apply_gradients(zip(gradient, model.trainable_weights)) 
    if epoch %5==0:
        print('epoch %d, loss %f'%(epoch,loss_value))

 
&lt;img class=&quot; w3-border&quot; src=&quot;/my-blog/assets/images/deep/logistic/keras.jpg&quot; /&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;/div&gt;

&lt;h2&gt; References:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Machine-Learning-Optimization-Perspective-Developers/dp/0128015225&quot; target=&quot;_blank&quot;&gt; Sergios Theodoridis. 
Machine Learning: A Bayesian
and Optimization Perspective.&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mitpress.mit.edu/books/machine-learning-1&quot; target=&quot;_blank&quot;&gt;Kevin P. Murphy. Machine Learning A Probabilistic Perspective.&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://d2l.ai/chapter_linear-networks/linear-regression.html&quot; target=&quot;_blank&quot;&gt;Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola.   Dive into Deep Learning.&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.springer.com/series/417&quot; target=&quot;_blank&quot;&gt; 
Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. An Introduction to Statistical Learning  with Applications in R.&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/tutorials&quot; target=&quot;_blank&quot;&gt;Tensorflow.&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/pytorch_with_examples.html&quot; target=&quot;_blank&quot;&gt; LEARNING PYTORCH WITH EXAMPLES.&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Arize Emmanuel</name></author><category term="deep-learning" /><summary type="html">In the previous section [Linear Regression], we talked about linear regression which deals with quatitative target variable, answering the questions how much? such as predicting the price of a house and the salary of an employee. In practice, we are more often interested in making categorical assignments by mapping an instance to a category of the target variable which may contain two or more different categories. Classification problems ask the question not how much? but which one (group or class) does an instance belongs to such as a boy or a girl. Classification problems include Spam e-mail filtering in which spam is taken as the positive class and ham as the negative class, comments classification where comments are classified as either positive or negative, medical diagnosis where having a particular disease is assigned the positive class and credit card fraud detection. We consider a problem to be a classification problem when dealing with qualitative (categorical) target variable with the aim of assigning an input described by vector \(X_{i}\) to one of the n discrete categories (classes) \(C_{i}\) where \(i = 1,2 \cdots,n\).</summary></entry><entry><title type="html">Linear Regression (DP)</title><link href="http://localhost:4000/my-blog/deeplearning/deep-learning/2021/01/16/Linear-Regression.html" rel="alternate" type="text/html" title="Linear Regression (DP)" /><published>2021-01-16T00:00:00+00:00</published><updated>2021-01-16T00:00:00+00:00</updated><id>http://localhost:4000/my-blog/deeplearning/deep-learning/2021/01/16/Linear-Regression</id><content type="html" xml:base="http://localhost:4000/my-blog/deeplearning/deep-learning/2021/01/16/Linear-Regression.html">&lt;p&gt;This tutorial is an introduction to linear regression analysis with an implementation in pytorch, mxnet and keras.&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Introduction&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;linear regression&lt;/b&gt; is a supervised learning algorithm and one of the most widely used model for regression analysis. In regression, we are interested in predicting a quantitative (continous) target (response) variable represented by \(Y\) from one or more predictors(features,inputs, explanatory variables) represented by \(X\). It asserts that the target variable is a linear function of the predictors.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Suppose&lt;/b&gt; as a data scientist in your company, you are tasked to predict
the impact of three advertising media (youtube, facebook and newspaper) on sales, because your comapny wants to know whether to invest in these media or not and if yes, which of these media has the highest impact on sales. Using machine learning techniques, our goal is to be able to generalized well on newly unseen data points and since the data which will be used in testing the model are usually not available, the available dataset are usually divided into training data or training set (usually 70-80% of the dataset) which will be used in training the model and the rest as testing set on which the trained model will be tested to determine the predicting power of the model becuase after the model has trained and deployed, it will be predicting similar (new) but unseen inputs. The rows of the dataset are called instances (observations or examples) and what we want to predict is called a target (label or dependent variable or response). The variables upon which the predictions are based are called features (covariates or independent variables).&lt;/p&gt;

&lt;p&gt; As a data scientist you may first ask&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Whether there is a relationship between the media and sales, and If there is a relationship, is it negative (negative in the sense that when one increases the other decreases and vice versa) or positive (when one increases the other increases).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Does these advertising media have high or low impact on sales and vice versa. How strong is the relationship between these features and sales. Which of these features has the highest contribution to sales increament.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is the relationship linear?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; Since we need some packages for the implementation of the model the code below imports the needed packages for this tutorials &lt;/p&gt;
&lt;div class=&quot;w3-container&quot;&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
import torch
import numpy as np
from torch.utils.data import DataLoader,TensorDataset
import pandas as pd
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
torch.manual_seed(100)
%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
%matplotlib inline
import d2l
from mxnet import autograd, np, npx,gluon
import mxnet
from pandas.plotting import scatter_matrix
import pandas as pd
import matplotlib.pyplot as plt
npx.set_np()
mxnet.random.seed(100)&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
%matplotlib inline
import numpy as np
import pandas as pd
from pandas.plotting import scatter_matrix
import tensorflow as tf
import matplotlib.pyplot as plt
tf.random.set_seed(100)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To determine the impact of the media on sales, we need data collected based on the previous amount of money invested into these media and their corresponding sales. In this tutorials the marketing dataset to used can be download here [&lt;a href=&quot;/my-blog/assets/data/marketing.csv&quot;&gt;Marketing Data&lt;/a&gt;]. After downloading the dataset placed it in a folder named data.&lt;/p&gt;
&lt;p&gt;Let now load the data and display the first four rows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
marketing=pd.read_csv(&quot;data/marketing.csv&quot;,
                       index_col=0)
marketing.head(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img class=&quot;w3-center w3-brown&quot; src=&quot;/my-blog/assets/images/deep/pytorch/marketing4row.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the dataset, let now explore the relationship between the media and sales through graphs.&lt;/p&gt;
&lt;p&gt;The graph between youtude and sales shows a strong positive relationship and for the company to increase sales it seems reasonably right to increase youtude advertisement&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;

plt.scatter(marketing.iloc[:, 0], 
                  marketing.iloc[:, 3], 2)
plt.xlabel(&quot;Youtude&quot;)
plt.ylabel(&quot;Sales&quot;)
plt.title(&quot;Youtude vrs Sales&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/keras/youtude.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt; The graph between facebook and sales shows a positive relationship which is not all that strong. Although the relationship is not strong but looking at the graph facebook advertisment is still likely to increase (the impact may not be high) sales &lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;

plt.scatter(marketing.iloc[:, 1],
                     marketing.iloc[:, 3],s=3);
plt.xlabel(&quot;Facebook&quot;)
plt.ylabel(&quot;Sales&quot;)
plt.title(&quot;Facebook vrs Sales&quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/keras/facebook.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There seems to be no relationship between newspaper and sales and that newspaper advertisment is likely to be a waste of money&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;

plt.scatter(marketing.iloc[:, 2], 
               marketing.iloc[:, 3],s=1)
plt.xlabel(&quot;News Paper&quot;)
plt.ylabel(&quot;Sales&quot;)
plt.title(&quot;News paper vrs Sales&quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;w3-center w3-brown&quot; src=&quot;/my-blog/assets/images/deep/keras/newspaper.jpg&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;scatter_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marketing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/keras/marketgrp.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The relationship between the features and sales can be confirmed through a correlation matrix&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
marketing.corr()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/keras/marketcorr.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;&lt;b&gt;Linear Model&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Using a linear model, it assert there is a linear relationship between the response denoted by &lt;b&gt;y&lt;/b&gt; and the features denoted by &lt;b&gt;X&lt;/b&gt; and that y can be expressed as a linear combination (weighted sum) of X  defined by:
&lt;br /&gt;&lt;/p&gt;

\[\hat{y} = XW+ \epsilon =\sum_{i=1}^{D}x_{i}w_{i}+ \epsilon\]

&lt;p&gt;where \(XW\) is the matrix-vector product (dot product) between X and W with &lt;b&gt;X&lt;/b&gt; repesenting the input vector, &lt;b&gt;W&lt;/b&gt; the model’s weight vector  and \(\epsilon\) the residual error between the model predictions and the true y values. &lt;b&gt; \(\epsilon\) &lt;/b&gt; is often assumed to be normally distributed and is denoted by \(\epsilon ∼N(\mu, \sigma^{2})\) where \(\mu\) is the mean and \(\sigma^{2}\) is the variance.&lt;/p&gt;

&lt;p&gt;Connecting linear regression and normal distirbustion, we can rewrite the model in the form:&lt;br /&gt;&lt;/p&gt;

\[p(y|x,\theta)=N(y|u(x),\sigma^{2}(x) )\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;and in the linear form \(u(x)\) can be expressed as
&lt;br /&gt;&lt;/p&gt;

\[\hat{y}=p(y|u(x),w)=u(x)=xw+b\]

&lt;p&gt;&lt;br /&gt;
  where &lt;b&gt;b&lt;/b&gt; is the bias term
&lt;br /&gt;
Using the marketing dataset in connection with the model we can express sales as a linear combination of the advertising media and is defined as:&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[Sales = w_{youtude} *youtude+ w_{facebook} * facebook + w_{newspaper}*newspaper + b\]

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
where \(w_{i}\) , i=youtude,facebook,newspaper represent weights associated with each advertising media and these weights determine the influence or impact of the media on sales. The bias or intercept b, gives us the expected predicted amount of sales when no adverts are made. With \(X \in R^{n*3}\)  representing the data points (media), \(y \in R^{n}\) the corresponding sales, \(W \in R^{3}\) the weight vector  and \(b \in R^{1}\) the bias term, the expected predicted sales (y)  can be expressed compactly via the matrix-vector product as&lt;/p&gt;

\[\hat{y} = XW+ b\]

&lt;p&gt;
Let now define the linear model and initialized it parameters w and b
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;w3-container&quot;&gt;
&lt;details&gt; &lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
def linreg(X, w, b):
    return torch.matmul(X.double(), w.double()) + b.double())
w=torch.normal(0,0.01,(3,1)).requires_grad_(True)
b= torch.zeros(size=(1,)).requires_grad_(True))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt; &lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
def linreg(X, w, b):
    return np.dot(X, w) + b
def init_params():
    w=np.random.normal(0,0.01,(3,1))
    b= np.zeros((1,))
    w.attach_grad()
    b.attach_grad()
    return [w,b]
w,b=init_params()&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt; &lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
def linreg(X, w, b):
    X=tf.dtypes.cast(X,dtype=tf.double)
    w=tf.dtypes.cast(w,dtype=tf.double)
    b=tf.dtypes.cast(b,dtype=tf.double)
    return tf.matmul(X, w) + b
w = tf.Variable(tf.random.normal(shape=(3, 1), 
             mean=0, stddev=0.01),trainable=True)
b = tf.Variable(tf.zeros(1), trainable=True))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p&gt;To determine how good or bad the model is generalizing, we need a performance measure to evaluate the model.&lt;/p&gt;

&lt;h3&gt;&lt;b&gt; Loss Function&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Our next step is to select a performance measure that will determine how good the model fit the data points by measuring the closeness of the predicted values to their real values. The loss function quantifies the distance between the actual or real and predicted value with a goal to minimize the prediction error. We will use mean sqaure error will as loss function.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Mean Squared Error (MSE)&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;MSE Measures the average of the squares of the errors. It tells us how close a 
fitted line (estimated values or predicted points) is to a set of data points (actual data points used in fitting the line) by calculating the distances between these data points and the fitted line. The disatnces calculated are known as the errors and MSE removes negative signs from the errors by squaring them, then find the average of these squared errors and is defined as&lt;br /&gt;&lt;/p&gt;

\[\ell(w,b)=\frac{1}{n} \sum_{1}^{n}( X_{i}W+b -y_{i})^{2}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[\ell(w,b) = \frac{1}{n}\sum_{i=1}^n (\hat{y}_i-y_i)^2.\]

&lt;p&gt;&lt;br /&gt;
during training we want to find the best parameters \((w∗,b∗)\) that minimize the total loss across all training samples:&lt;br /&gt;&lt;/p&gt;

\[w∗, b∗ = argmin_{w,b}L(w, b)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Let define the mean squared error function&lt;/p&gt;
&lt;div class=&quot;w3-container&quot;&gt;
&lt;br /&gt;
&lt;details&gt;&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
def Mean_squared_loss(y_hat, y):
    squared_def=(y_hat - y.reshape(y_hat.shape)) ** 2 
    return squared_def.mean()
&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;&lt;/details&gt;&lt;br /&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
def Mean_squared_loss(y_hat, y):
    squared_def=(y_hat - y.reshape(y_hat.shape)) ** 2 
    return squared_def.mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;&lt;br /&gt;

&lt;details&gt;&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
def Mean_squared_loss(y_hat, y):
    y_hat=tf.dtypes.cast(y_hat,dtype=tf.double)
    y=tf.dtypes.cast(y,dtype=tf.double)
    squared_def=(y_hat - tf.reshape(y,y_hat.shape)) ** 2 
    return tf.reduce_mean(squared_def)
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;b&gt; Updating the Model Parameters&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;During trainiing we want to automatically update the model parameters in order to the find the best parameters that minimize the error and hence We will use Gradient descent in updating the parameters.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt; Gradient Descent&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;Gradient Descent is a generic optimization algorithm capable of finding the optimal solutions to a wide range of problems with th goal of iteratively reducing the error by tweaking (updating) the parameters in the direction that incrementally lowers the loss value.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt; The Delta Rule&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;Now suppose we have \(\hat y=u=w_{1}x\)&lt;/p&gt;

&lt;p&gt;then the error is given as&lt;/p&gt;

\[E=y- \hat y\]

&lt;p&gt;and the square error \(\epsilon\) for an input is given as&lt;/p&gt;

\[\epsilon =\frac{1}{2}E^{2}=\frac{1}{2}(y- \hat y)^{2}\]

&lt;p&gt;The fraction 1⁄2 is arbitrary, and is used for mathematical convenience&lt;/p&gt;

&lt;p&gt;To find the gradient at a particular weight we differentiate the square error function with respect to that particular weight, yielding&lt;/p&gt;

\[\frac{d \epsilon}{dw_{1}} =\frac{2}{2}(y- w_{1}x)(-x)=-Ex\]

&lt;p&gt;&lt;b&gt;Using the delta rule&lt;/b&gt;, a change in weight is proportional to the negative of the error gradient because it is necessary to go down the error curve in order to minimize the error; therefore, the weight change is given by:&lt;/p&gt;

\[\Delta w_{1}\propto -Ex\]

&lt;p&gt;where \(\propto\) denotes proportionality, which can be replaced with the learning rate \(\beta\). The learning rate is a tuning parameter which determines how big or small a step is taking along the gradient in order to find a better weight. A too high value for \(\beta\) will make the learning jump over the minima and smaller values slow it down and will take too long to converge.Using \(\beta\) the eqution becomes&lt;/p&gt;

\[\Delta w_{1}=-\beta Ex\]

&lt;p&gt;The new weight after the ith iteration canbe expressed as&lt;/p&gt;

\[W_{1}^{i+1}=W_{1}^{i}- \Delta w_{1}=W_{1}^{i}-\beta Ex\]

&lt;p&gt;using the same idea we can extend to &lt;br /&gt;&lt;/p&gt;

\[W_{j}^{i+1}=W_{j}^{i}-\Delta w_{j}^{i}=W_{j}^{i}-\beta Ex_{j}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[W_{0}^{i+1}=W_{0}^{i}-\beta E\]

&lt;p&gt;&lt;br /&gt;
where \(W_{0}\) is the bias&lt;/p&gt;

&lt;h3&gt;&lt;b&gt; stochastic gradient descent(SGD)&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;When training the model, it is preferable to use the whole training set to compute the gradients at every step and this is known as &lt;b&gt;Batch Gradient Descent&lt;/b&gt;  but this can slow the training process of the model when the training set is large so instead we mostly used a random sample of the training set in updating the parameters and this method is known as &lt;b&gt;stochastic gradient descent&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;stochastic gradient descent&lt;/b&gt; is a stochastic approximation proximation of the gradient descent with the fact that the gradient is precisely not know, but instead we make a rough approximation to the gradient in order to minimize the objective function which is written as a sum of differentiable functions&lt;/p&gt;

\[\Delta w_{1}=\beta (\frac{1}{n} \sum^{n}Ex_{i})\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;where n is the batch size&lt;/p&gt;

&lt;p&gt;The code below defines SGD and this will be used in updating the model the parameters&lt;/p&gt;

&lt;div&gt; 
&lt;br /&gt;
&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
def sgd(params,lr,batch_size):
    for param in params:
        param.data.sub_(lr*param.grad/batch_size)
        param.grad.data.zero_()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
def sgd(params,lr,batch_size):
    for param in params:
        param[:]=param-lr*param.grad/batch_size
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
def sgd(params,grads,lr,batch_size):
    for param ,grad in zip(params,grads):
        param.assign_sub(lr*grad/batch_size)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;
 &lt;/div&gt;

&lt;h3&gt;&lt;b&gt;Feature Scaling or transformation&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Numerical features are often measured on different scales and this variation in scale may pose problems to modelling the data correclty. With few exceptions, variations in scales of numerical features often lead to machine Learning algorithms not performing well. When features have different scales, features with higher magnitude are likely to have higher weights and this affects the performance of the model.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Feature scaling is a technique applied as part of the data preparation process in machine learning to put the numerical features on a common scale without distorting the differences in the range of values.&lt;/em&gt;
There are many feature scaling techniques such as&lt;/p&gt;
&lt;h3&gt;
       &lt;b&gt; Min-max scaling (normalization)&lt;/b&gt;&lt;h3&gt;

$$ x_{unit-interval}= \frac{x-min(x)}{max(x)-min(x)}$$


&lt;h3&gt;
       &lt;b&gt;Box-Cox transformation.&lt;/b&gt;&lt;/h3&gt;

$$ x_{box-cox}= \frac{x^{\lambda}-1}{ \lambda }$$

      
&lt;h3&gt;
       &lt;b&gt;Mean normalization.&lt;/b&gt;&lt;/h3&gt;

$$ x_{mean-normalization}= \frac{x-\bar{x}}{max(x)-min(x)}$$

      
  etc, but We will discuss only &lt;b&gt;Standardization(Z-score)&lt;/b&gt; 
 
 &lt;h3&gt;  Standardization(Z-score)&lt;/h3&gt; 
When Z-score is applied to a input feature makes the  feature have a zero mean by subtracting the mean of the feature from the data points and then it divides by the standard deviation so that the resulting distribution has unit variance.
 
&lt;h3&gt;
       &lt;b&gt;Standardization.&lt;/b&gt;&lt;/h3&gt;
$$ x_{z-score}= \frac{x-\bar{x}}{ \sigma}$$


since all features (advertising media) in our dataset are numrical we are going to scale them using Z-score

&lt;p&gt;&lt;b&gt;Note: the target values is generally not scaled&lt;/b&gt;&lt;/p&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
features=marketing.iloc[:,0:3]
features=features.apply(lambda x: (x-np.mean(x))/np.std(x))
&lt;/code&gt;
&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;

&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
features=marketing.iloc[:,0:3]
features=features.apply(lambda x: (x-x.mean())/x.std())
&lt;/code&gt;
&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;


&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python},&quot;&gt;
features=marketing.iloc[:,0:3]
features=features.apply(lambda x: (x-np.mean(x))/np.std(x))
&lt;/code&gt;
&lt;/pre&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;p&gt;  The code below display the first three rows of the scaled input features&lt;/p&gt;
```python
features.head(3)
```
&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/keras/scaledmarket.jpg&quot; /&gt;&lt;/p&gt;


&lt;h3&gt;&lt;b&gt; Data iterators&lt;/b&gt;&lt;/h3&gt;
 To update parameters we need to iterate through our data points and grab batches of the data points at a time and these batches are used to update the parameters&lt;br /&gt;

&lt;div&gt; 
&lt;br /&gt;
&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
def data_iter(features,labels,batch_size):
    dataset=TensorDataset(*(features,labels))
    dataloader=DataLoader(dataset=dataset,batch_size=batch_size,
                          shuffle=True)
    return dataloader
features=torch.from_numpy(np.array(features,dtype=np.float32))
sales=torch.from_numpy(np.array(marketing.iloc[:, 3],dtype=np.float32))
batch_size=10
data_iter=data_iter(features, sales,batch_size))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
def data_iter(data_arrays, batch_size):
    dataset=gluon.data.ArrayDataset(*data_arrays)
    dataloader=gluon.data.DataLoader(dataset,batch_size,shuffle=True)
    return dataloader
features=np.array(features)
sales=np.array(marketing.iloc[:, 3])
batch_size = 10
data_iter = data_iter((features,sales), batch_size)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;

&lt;details&gt; 
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;
batch_size=10
def data_iter(features,labels, batch_size):
    dataset=tf.data.Dataset.from_tensor_slices((features,labels))
    dataloader=dataset.shuffle(buffer_size=500).batch(batch_size)
    return dataloader
features=tf.convert_to_tensor(np.array(marketing.iloc[:,0:3]))
data_iter = data_iter(features,sales, batch_size))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;&lt;br /&gt;
 &lt;/div&gt;


&lt;p&gt; Let now train the model&lt;/p&gt;
&lt;div class=&quot;w3-container&quot;&gt;
&lt;br /&gt;&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;PYTORCH&lt;/summary&gt;
&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/linear/pytorch.jpg&quot; /&gt;
&lt;/details&gt;&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;MXNET&lt;/summary&gt;
&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/linear/mxnet.jpg&quot; /&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;details&gt;
&lt;summary class=&quot;w3-button&quot;&gt;KERAS&lt;/summary&gt;
&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/deep/linear/keras.jpg&quot; /&gt;
&lt;/details&gt;
&lt;br /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;/h3&gt;
In this we looked at Linear regression and when to perform linear regression analysis. Before training the model on the dataset, the numerical features have to be scaled for them to fall within a common range and lastly we need to pick a lost function which will quantify how good or bad the model is.


&lt;h2&gt; References:&lt;/h2&gt;

- &lt;a href=&quot;https://www.amazon.com/Machine-Learning-Optimization-Perspective-Developers/dp/0128015225&quot; target=&quot;_blank&quot;&gt; Sergios Theodoridis. 
Machine Learning: A Bayesian
and Optimization Perspective.
&lt;/a&gt;&lt;br /&gt;



- &lt;a href=&quot;https://mitpress.mit.edu/books/machine-learning-1&quot; target=&quot;_blank&quot;&gt;Kevin P. Murphy. Machine Learning A Probabilistic Perspective.&lt;/a&gt;&lt;br /&gt;
- &lt;a href=&quot;http://d2l.ai/chapter_linear-networks/linear-regression.html&quot; target=&quot;_blank&quot;&gt;Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola.   Dive into Deep Learning.&lt;/a&gt;&lt;br /&gt;
- &lt;a href=&quot;https://www.springer.com/series/417&quot; target=&quot;_blank&quot;&gt; 
Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. An Introduction to Statistical Learning  with Applications in R.&lt;/a&gt;&lt;br /&gt;
- &lt;a href=&quot;https://www.tensorflow.org/tutorials&quot; target=&quot;_blank&quot;&gt;Tensorflow.&lt;/a&gt;&lt;br /&gt;
- &lt;a href=&quot;https://pytorch.org/tutorials/beginner/pytorch_with_examples.html&quot; target=&quot;_blank&quot;&gt; LEARNING PYTORCH WITH EXAMPLES.&lt;/a&gt;&lt;br /&gt;

&lt;/h3&gt;&lt;/h3&gt;</content><author><name>Arize Emmanuel</name></author><category term="deep-learning" /><summary type="html">This tutorial is an introduction to linear regression analysis with an implementation in pytorch, mxnet and keras. Introduction linear regression is a supervised learning algorithm and one of the most widely used model for regression analysis. In regression, we are interested in predicting a quantitative (continous) target (response) variable represented by \(Y\) from one or more predictors(features,inputs, explanatory variables) represented by \(X\). It asserts that the target variable is a linear function of the predictors.</summary></entry><entry><title type="html">Cross-Validation(R)</title><link href="http://localhost:4000/my-blog/machine-learning-r/2021/01/14/cross-validation.html" rel="alternate" type="text/html" title="Cross-Validation(R)" /><published>2021-01-14T02:02:23+00:00</published><updated>2021-01-14T02:02:23+00:00</updated><id>http://localhost:4000/my-blog/machine-learning-r/2021/01/14/cross-validation</id><content type="html" xml:base="http://localhost:4000/my-blog/machine-learning-r/2021/01/14/cross-validation.html">&lt;p&gt;One of the finest techniques to check the generalization power of a machine learning model is to use &lt;strong&gt;&lt;em&gt;Cross-validation techniques&lt;/em&gt;&lt;/strong&gt;. &lt;strong&gt;Cross-validation&lt;/strong&gt; refers to a set of methods for measuring the performance of a given predictive model and can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. Cross-validation techniques generally involves the following process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;DividE the available data set into two sets namely training and testing (validation) data set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Train the model using the training set&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Test the effectiveness of the model on the the reserved sample (testing) of the data set and estimate the prediction error.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;cross-validation methods for assessing model performance includes,&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     Validation set approach (or data split)
     Leave One Out Cross Validation
     k-fold Cross Validation
     Repeated k-fold Cross Validation
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;validation-set-approach&quot;&gt;Validation Set Approach&lt;/h1&gt;
&lt;p&gt;The validation set approach involves&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 1.  randomly dividing the available data set into two parts namely,  training data set and validation data set.

 2.  Model is trained on the training data set

 3.  The Trained model is then used to predict observations in the validation   set to test the generalization 
   
   ability of  the model when faced with new observations by calculating the prediction error using model 
   
   performance metrics
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; loading the needed libraries &lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(tidyverse)
library(caret)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Loading the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;data(&quot;marketing&quot;, package = &quot;datarium&quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;cat(&quot;The advertising dataset has&quot;,nrow(marketing),'observations and',ncol(marketing),'features')

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The advertising datasets has 200 observations and 4 features&lt;/p&gt;

&lt;p&gt;displaying the first four rows or observations of the dataset&lt;/p&gt;
&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marketing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/rmarketing.jpg&quot; /&gt;&lt;/p&gt;</content><author><name>Arize Emmanuel</name></author><category term="machine-learning-R" /><category term="very good tutorials" /><summary type="html">One of the finest techniques to check the generalization power of a machine learning model is to use Cross-validation techniques. Cross-validation refers to a set of methods for measuring the performance of a given predictive model and can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. Cross-validation techniques generally involves the following process:</summary></entry><entry><title type="html">Cross-Validation (ML-python)</title><link href="http://localhost:4000/my-blog/machine-learning-python/2021/01/14/cross-validation.html" rel="alternate" type="text/html" title="Cross-Validation (ML-python)" /><published>2021-01-14T02:02:23+00:00</published><updated>2021-01-14T02:02:23+00:00</updated><id>http://localhost:4000/my-blog/machine-learning-python/2021/01/14/cross-validation</id><content type="html" xml:base="http://localhost:4000/my-blog/machine-learning-python/2021/01/14/cross-validation.html">&lt;p&gt;One of the finest techniques to check the generalization power of a machine learning model is to use &lt;strong&gt;&lt;em&gt;Cross-validation techniques&lt;/em&gt;&lt;/strong&gt;. &lt;strong&gt;Cross-validation&lt;/strong&gt; refers to a set of methods for measuring the performance of a given predictive model and can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. Cross-validation techniques generally involves the following process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;DividE the available data set into two sets namely training and testing (validation) data set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Train the model using the training set&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Test the effectiveness of the model on the the reserved sample (testing) of the data set and estimate the prediction error.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;cross-validation methods for assessing model performance includes,&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     Validation set approach (or data split)
     Leave One Out Cross Validation
     k-fold Cross Validation
     Repeated k-fold Cross Validation
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;validation-set-approach&quot;&gt;Validation Set Approach&lt;/h1&gt;
&lt;p&gt;The validation set approach involves&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; 1.  randomly dividing the available data set into two parts namely,  training data set and validation data set.

 2.  Model is trained on the training data set

 3.  The Trained model is then used to predict observations in the validation   set to test the generalization 
   
   ability of  the model when faced with new observations by calculating the prediction error using model 
   
   performance metrics
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; importing the needed packages &lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{python}&quot;&gt;from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,LeaveOneOut,KFold,RepeatedKFold
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer,mean_squared_error,r2_score
import sklearn
import pandas as pd

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Loading the data&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;marketing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data/marking.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;The advertising datasets has &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marketing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' observations and '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marketing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' features'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The advertising datasets has 200 observations and 4 features&lt;/p&gt;

&lt;p&gt;displaying the first four rows or observations of the dataset&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;marketing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/python/images/marketing.jpg&quot; style=&quot;width:300px;height:200px;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;feature-scaling-or-transformation&quot;&gt;Feature Scaling or transformation&lt;/h3&gt;

&lt;p&gt;Numerical features are often measured on different scales and this variation in scale may pose problems to modelling the data correclty. With few exceptions, variations in scales of numerical features often lead to machine Learning algorithms not performing well. When features have different scales, features with higher magnitude are likely to have higher weights and this affects the performance of the model.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Feature scaling is a technique applied as part of the data preparation process in machine learning to put the numerical features on a common scale without distorting the differences in the range of values.&lt;/em&gt;
There are many feature scaling techniques such as&lt;/p&gt;

&lt;p class=&quot;w3-panel w3-border w3-leftbar w3-center w3-rightbar w3-round-xlarge&quot;&gt;
       &lt;b&gt; Min-max scaling (normalization)&lt;/b&gt;
$$ x_{unit-interval}= \frac{x-min(x)}{max(x)-min(x)}$$
&lt;/p&gt;

&lt;p class=&quot;w3-panel w3-border w3-leftbar w3-center w3-rightbar w3-round-xlarge&quot;&gt;
       &lt;b&gt;Box-Cox transformation.&lt;/b&gt;
$$ x_{box-cox}= \frac{x^{\lambda}-1}{ \lambda }$$
&lt;/p&gt;

&lt;p class=&quot;w3-panel w3-border w3-leftbar w3-center w3-rightbar w3-round-xlarge&quot;&gt;
       &lt;b&gt;Mean normalization.&lt;/b&gt;
$$ x_{mean-normalization}= \frac{x-\bar{x}}{max(x)-min(x)}$$

&lt;/p&gt;

&lt;p&gt;etc, but We will discuss only &lt;b&gt;Standardization(Z-score)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;## Standardization(Z-score) 
When Z-score is applied to a feature this makes the feature have a zero mean by subtracting the mean of the feature from the data points and then it divides by the standard deviation so that the resulting distribution has unit variance.&lt;/p&gt;

&lt;p class=&quot;w3-panel w3-border w3-leftbar w3-center w3-rightbar w3-round-xlarge&quot;&gt;
       &lt;b&gt;Standardization.&lt;/b&gt;
$$ x_{z-score}= \frac{x-\bar{x}}{ \sigma}$$
&lt;/p&gt;

&lt;p&gt;since all features in our dataset are numrical we are going to scale them using Z-score&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Note: the target values is generally not scaled&lt;/b&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marketing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img class=&quot;w3-center&quot; src=&quot;/my-blog/assets/images/python/scale_market.jpg&quot; /&gt;&lt;/p&gt;</content><author><name>Arize Emmanuel</name></author><category term="machine-learning-python" /><summary type="html">One of the finest techniques to check the generalization power of a machine learning model is to use Cross-validation techniques. Cross-validation refers to a set of methods for measuring the performance of a given predictive model and can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. Cross-validation techniques generally involves the following process:</summary></entry></feed>