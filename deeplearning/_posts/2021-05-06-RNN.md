---
layout: post-deep-learning
title: Recurrent Neural Networks (RNNs)
category: deep-learning
editor_options: 
  markdown: 
    wrap: 72
---
A major characteristic of feedforward networks is that these networks take in arbitrary feature vectors with fixed, predetermined input sizes, along with their associated weights and had no hidden state. With feedforward networks, in order to process a sequence of data points, the entire sequence is considered as a single input for the network to process and capture all relevant information at once in a single step. This makes it difficult to deal with sequences of varying length and fails to capture important information. Sequential data usually involve variable lenght inputs, so instead of processing the data point in just a single step, we need a model that will still consider a sequence as a single input to the network but instead of processing it in a single step, the model will internally loop over the sequence elements taking each element as an input and maintaining a state containing information relative to what it has seen so far and this is the ideal behind RNNS.

<b style="text-decoration:underline;font-size: 20px;text-transform: uppercase;">Recurrent neural networks or RNNs</b> are networks containing recurrent connections within their network connections and are often used for processing sequential data.   RNNs assumes that an incoming data take the form of a sequence of vectors or tensors, which can be sequences of word or sequences of characters as in textual data, sequence of observations over a period of time as in time series etc.  

An a assumption of feedforward networks is that the inputs are independent (one input has no dependency on another) of one another , however in sequential data such as textual data (we will limit ourselves to the most widespread forms of sequence data which is textual data), this assumption is not true, since in a sentence the occurrence of a word influences or is influenced by the occurrences of other words in the sentence. RNNs have recurrent connections that allows a memory to persist in the network’s internal state keeping track of information observed so far and informing the decisions to be made by the network at later points in time and also share parameters across different parts of the network making it possible to extend and apply the network to inputs of variable lengths and generalize across them, this makes RNNs useful for Timeseries forecasting and natural language processing (NLP) systems such as document classification, sentiment analysis, automatic translation, generating text for applications such as chatbots. Since the same parameters are used for all time steps, the parameterization cost of an RNN does not grow as the number of time steps increases.

From the book <a href='https://www.deeplearningbook.org/contents/rnn.html' target="_blank">Deep Learning-chapter 10</a>

> Some examples of important design patterns for recurrent neural networks
include the following:
 - Recurrent networks that produce an output at each time step and have
recurrent connections between hidden units.
 - Recurrent networks that produce an output at each time step and have
recurrent connections only from the output at one time step to the hidden
units at the next time step.
 - Recurrent networks with recurrent connections between hidden units, that
read an entire sequence and then produce a single output.


In this section, we will  consider a class of recurrent networks referred to as Elman Networks (Elman,1990) or simple recurrent networks which serve as the basis for more complex approaches like the Long Short-Term Memory (LSTM) networks and Gated Recurrent Unit (GRU). A simple RNN which is typically a three-layer network comprising an input layer, a single hidden layer and an output layer.   

<a  style="color:blue;" href="#Fig">Figure 1</a> is a diagramatic view of RNN with  input to hidden connections parametrized by a weight matrix $$ U \in R^{d \times h} $$, hidden-to-hidden recurrent connections parametrized by a weight matrix $$W\in R^{h \times h}$$ and hidden-to-output connections parametrized by a weight matrix  $$ V \in R^{h*o} $$ and $$ h \in R^{n*h} $$ representing the hidden state of the network.  On the Left side is RNN drawn with recurrent connections and on the Right is the same RNN seen as an time unfolded computational graph, where each node is now associated with one particular time instance and this illustrate the computational logic of an RNN at adjacent time steps.


<img class="w3-center" src="{{'/assets/images/deep/keras/rnn.png' |relative_url}}">
<br/>
<span id='Fig'>Figure 1</span><br/>
<a href="https://www.google.com/search?q=rnn+image&client=firefox-b-d&tbm=isch&source=iu&ictx=1&fir=lD-kwEF8OCJIoM%252C5nGST21LG70DyM%252C_&vet=1&usg=AI4_-kTE51-vQdo1Mb1V3I10kNw5Xv3yAw&sa=X&ved=2ahUKEwir7rW18sjuAhVOXMAKHSm_CMQQ9QF6BAgHEAE&biw=1366&bih=580#imgrc=8TAzbbCVWa8qZM">source: RNN</a>
<br/>
Most RNNs computation  can be decomposed into three blocks of parameters and associated transformations or activation function:
- 1. from the input to the hidden state,
- 2. from the previous hidden state to the next hidden state, and
- 3. from the hidden state to the output


Armed with a summary of RNNs computational decomposition, let assume we have a minibatch of inputs $$X^{t} \in R^{n×d}$$ where each row of $$X^{t}$$ corresponds to one example at time step ***t*** from the sequence and  $$ h^{t} \in R^{n*h}$$ be  the hidden state at time ***t***. 
 Unlike standard feedforward networks, RNNs current hidden state $$h^{t}$$ is a function $$\phi$$ of the previous hidden state $$h^{t-1}$$ and the current input $$x^{t}$$ defined by $$h^{t}=\phi(x^{t}U+ h^{t-1}W+b^{h} )$$ where  $$b^{h} \in R^h$$ is the bias term and the weights $$W$$ determine how the network current state makes use of past context in calculating the output for the current input.


The fact that the computation of the hidden state at time t requires the value of the hidden state from time t −1 mandates an incremental inference algorithm that proceeds from the start of the sequence to the end and thus RNNs condition the next word on the sentence history. With $$h^{t}$$ defined, RNN is defined as function $$\phi$$ taking as input a state vector $$h^{t-1}$$ and an input vector $$x^{t}$$ and return a new state vector $$h^{t}$$. The initial state vector $$h^{0}$$, is also an input to the RNN, assumed to be a zero vector and often omitted. The hidden state $$h$$ is then used as the input for the output layer and is given by


$$h^{t}=\phi(X^{t}U+ h^{t-1}W+b^{h} )$$

 $$ O=f(h^{t}V +b^{o})$$

  $$\hat y=\phi (O)$$
  

 where $$O \in R^{n \times o}$$ and  $$b^{o} \in R^{o}$$
 
 Layers performing $$h^{t}=\phi(x^{t}U+ h^{t-1}W+b^{h} )$$  in RNNs are called recurrent layers.
 
<a style="color:blue" href="https://github.com/emmanuel-arize/Deep-learning-with-keras/blob/master/notebook/RNN/INTRODUCTION_TO_RNN_(SimpleRNN)_MOVIES_REVIEWS.ipynb">Simple RNN implementation with Keras</a>


 
<p> <b>References:</b></p>
-<a href='https://www.deeplearningbook.org/contents/rnn.html'>
Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016). Deep Learning. MIT Press,pp.389-413
</a>

<a href="https://link.springer.com/article/10.1007/BF00114844">Elman, J. L. (1991). Distributed representations, simple recurrent networks, and grammatical structure. Machine learning, 7(2), 195-225.</a>

<a href="https://arxiv.org/pdf/1412.7753.pdf">Mikolov, T., Joulin, A., Chopra, S., Mathieu, M., & Ranzato, M. A. (2014). Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753.</a>