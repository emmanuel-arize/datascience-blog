---
layout: post-python
title: Feature Extraction with Bag of Words (BOWs)
category: machine-learning-python
---
In this tutorial, we will learn about Bag of Words  (BOWs), how BOWs is used as feature extractor then build a classifier using the features extracted.

In order to model a text documents, the raw text cannot be fed directly to the algorithm as these algorithms expect numerical feature vectors so instead we need to turn the text content into numerical feature vectors.

<span class='w3-text-blue'> From the [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html):</span>
<b>
We call vectorization the general process of turning a collection of text documents into numerical feature vectors.
</b>

When modelling a data it is important to decide what features of the input are relevant, and how to encode those features. When we consider a textual data such as a sentence or a document  for instance the observable features are the counts and the order of the letters and the words within the text and as such we a way to extract these features from the text. There are several ways of extracting features from a textual data but in this tutorial we will only consider a very common feature extraction procedures for sentences and documents known as the <b> bag-of-words approach (BOW)</b> which looks at the histogram of the unique words within the text ( considering each word count as a feature.) 


<p><b>Bag Of Words (BOWs) Approach</b></p> 
Is a feature extraction technique used for extracting features from textual data and is commonly used in problems such as language modeling and document classification.  A bag-of-words is a representation of textual data, describing the occurrence of words within a sentence or document, disregarding grammar and the order of words.

<p><b>How does Bag of Words Works</b></p>
In order to understand how bag of words works let consider the two simple text documents:

```
1. Boys like playing football and Emma is a boy so Emma likes playing football

2  Mary likes watching movies 

```

Based on these two text documents, a list of token (words) for each document is as follows

```javascript
['Boys', 'like', 'playing', 'football', 'and', 'Emma', 'is' 'a', 'boy', 'so', 'Emma', 

'likes', 'playing', 'football']


['Mary', 'likes', 'watching', 'movies']


```
denoting document 1 by doc1 and 2  by doc2, we will construct a dictionary (key->value pair) of
words for both doc1 and doc2 where each key is a word, and each value is the number of occurrences of that word in the given text document.


```javascript
doc1={ 'a' : 1, 'and' : 1, 'boy' : 1, 'Boys' : 1, 'Emma' : 2, 'football' : 2, 

'is' : 1, 'like' : 1,  'likes' : 1, 'playing' : 2,   'so' : 1}

dco2={'likes' : 1, 'Mary' : 1,  'movies' : 1 ,'watching' : 1}
```

<b>NOTE :</b> the order of the words is not important


considering **a** as a stop word, we first define our vocabulary words, which is the set of all unique words found in our document set and it consist of
```javascript
and, boy, boys, emma,  football, is, like, likes, mary, movies, playing, so, watching

```
and the  features extracted using bag of words for the document set will be


<img class=" w3-border" src="{{'/assets/images/python/bog.jpg' |relative_url}}">


<p><b>scikit-learn CountVectorize implementation</b></p>
<p>
Using CountVectorize the text is preprocessed, tokenize and stopwords are filtered, it then builds a dictionary of features and transforms documents to feature vectors:</p>

```python
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
```

```
docs=['Boys like playing football and Emma is a boy so Emma likes playing football',
   "Mary likes watching movies"]
```
```python
feature_extr=CountVectorizer()
model=feature_extr.fit_transform(docs)
```


<img class=" w3-border" src="{{'/assets/images/python/bog1.jpg' |relative_url}}">

<p><b>Disadvantages</b></p>
Although BOWs is very simple to understand and implement, it has some disadvantages which include

- highly sparse vectors or matrix as the are  very few non-zero elements in dimensions corresponding to words that occur in the sentence.
- Bag of words representation leads to a high dimensional feature vector as the total dimension is the vocabulary size.
- Bag of words representation does not consider the semantic relation between words by assuming that the words are independent of each other.

<p><b> Buiding a Classifier with the features extracted using BOWS</b></p>

```python
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
```
The dataset called “Twenty Newsgroups”. which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. <a href='http://qwone.com/~jason/20Newsgroups/'>Official description of theTwenty Newsgroups data</a> will be used as our data but instead of the 20 different groups we will work on a partial dataset with only 11 categories out of the 20 available in the dataset. The code below is list of the 11 categories.

```python
categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med','sci.electronics',
              'sci.space','talk.politics.guns','talk.politics.mideast','talk.politics.misc',
              'talk.religion.misc','misc.forsale']
```

Loading the training data
```python
twenty_news_train=fetch_20newsgroups(subset='train',categories=categories,remove=('footers','headers','quotes'))
```
Let’s print the third lines of the first loaded file and the first four categories  with the target variable  :

<img class=" w3-border" src="{{'/assets/images/python/bog2.jpg' |relative_url}}">

<p><b>A Classifier Pipeline </b></p>

In order to make the vectorizer => classifier easier to work with, let build a pipeline with  a regularized linear models with stochastic gradient descent (SGD) learning as our classifier as below:

```python
news_clf.fit(twenty_news_train.data,twenty_news_train.target)
```
Let now train the classifier 
```python
news_clf.fit(twenty_news_train.data,twenty_news_train.target)
```
We now test the classifier on new instances 

<img class=" w3-border" src="{{'/assets/images/python/bog3.jpg' |relative_url}}">

<p><b>Evaluation of the performance on the test set</b></p>

```python
twenty_news_test=fetch_20newsgroups(subset='test',categories=categories,remove=('footers','headers','quotes'))
```
<img class=" w3-border" src="{{'/assets/images/python/bog4.jpg' |relative_url}}">




<p> <b>References:</b></p>
<hr>
- <a href='https://en.wikipedia.org/wiki/Bag-of-words_model' target="_blank">Bag-of-words model - Wikipedia
</a><br>
- <a href="https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/ref=as_li_ss_tl?ie=UTF8&qid=1502062931&sr=8-1&keywords=Neural+Network+Methods+in+Natural+Language+Processing&linkCode=sl1&tag=inspiredalgor-20&linkId=d63df073fea3ebe2d405820570b3ff03" target="_blank">Yoav Goldberg (2017). Neural Network Methods in Natural Language Processing</a><br>


- <a href="https://scikit-learn.org/stable/modules/feature_extraction.html" target="_blank">Feature extraction-scikit-learn documentation</a><br>

